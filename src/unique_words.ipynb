{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unique words in the budget\n",
    "\n",
    "This notebook documents the prevalence of unique words in the \n",
    "budget.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np \n",
    "\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import corpora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from budget_corpus import read_raw_corpus, read_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_corpus = read_raw_corpus()\n",
    "corpus = read_documents()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-02-27 21:05:26,259 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-02-27 21:05:26,317 : INFO : built Dictionary(4392 unique tokens: ['acquisition', 'aircraft', 'authorize', 'capital', 'derive']...) from 1248 documents (total 69715 corpus positions)\n"
     ]
    }
   ],
   "source": [
    "dictionary = corpora.Dictionary(tokens for tokens in corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## No frequest words left to filter\n",
    "\n",
    "My stopwords set includes a number of legalese and budget\n",
    "words.  See budget_corpus.py. \n",
    "\n",
    "This statement keeps all the words only used once (because\n",
    "butterfly was only used once. It would delete words that appear\n",
    "in over 50% of the documents, but there aren't any. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-02-27 21:05:26,324 : INFO : discarding 0 tokens: []...\n",
      "2019-02-27 21:05:26,326 : INFO : keeping 4392 tokens which were in no less than 0 and no more than 624 (=50.0%) documents\n",
      "2019-02-27 21:05:26,329 : INFO : resulting dictionary: Dictionary(4392 unique tokens: ['acquisition', 'aircraft', 'authorize', 'capital', 'derive']...)\n"
     ]
    }
   ],
   "source": [
    "dictionary.filter_extremes(no_below=0, no_above=.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert documents to tokens \n",
    "\n",
    "And see how many documents contain a word that only occurs\n",
    "once in the budget."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokened_corpus = [dictionary.doc2bow(words) for words in corpus ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1685"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "odd_tokens = []\n",
    "for token,freq in dictionary.dfs.items():\n",
    "    if freq == 1:\n",
    "        odd_tokens.append(token)\n",
    "len(odd_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Too many unique words to print - let's do 20 lines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "moderately focused random randomized landlord promising forbid apprentice trainee unemployed bid delay \n",
      "postage suffrage deobligation rehire wellness dump heroin willing nonagricultural nonimmigrant committed \n",
      "practical liaison accessory exporter shipper barrel cylinder frame breech postmaster canadian maintained \n",
      "escalation helsinki reinsurance turkey turkish indictment stand coalition procedural elizabeth beach \n",
      "downgrade resiliency telemarketing correspondingly manifest packing grape varietal wine microorganism \n",
      "kindred dune retitle lakeshore nonapplication douglas redesignation miller assurance supreme cotton avian \n",
      "specialty zoonotic stockpile scrapie screwworm formulate brucellosis escort philosophical aids improper \n",
      "analytic confer alignment earlier afford roma escobare grulla salineno les multiply redistribution burned \n",
      "furnished capitalized retardant equality raise setting marriage genital cutting mutilation constrain \n",
      "destabilizing neighboring abet peacebuilding lord uganda warning chad cameroon niger nigeria boko haram \n",
      "malawi ebola cancele darfur blue nile abyei referendum viable macroeconomic elephant rhinoceros tiger \n",
      "ape turtle healthplan refuse compliant adjudicate entrepreneurial microloan radicalization phenomenon \n",
      "root predatory behavior victimization restorative dismantlement unadopted generic biosimilar imported \n",
      "biologic oak scientist biotechnology nutritional mammography recall reinspection importer outsourcing \n",
      "distributor licensing monograph redecorate redecoration suite differential equipped spectrum sarbane \n",
      "oxley beginning programming unencumbered nonproject billable abandoned acid uncertified lander jupiter \n",
      "europa planetary decadal circulate coin numismatic coinage magistrate ocelot leopardu pardali alabama \n",
      "arizona arkansa colorado connecticut delaware illinois iowa louisiana maine massachusett michigan minnesota \n",
      "montana hampshire dakota ohio pennsylvania rhode utah vermont wisconsin wyoming marijuana nonrepayable \n",
      "refurnishing heating lighting cache uinta wasatch toiyabe angele bernardino sequoia cleveland ozark francis \n"
     ]
    }
   ],
   "source": [
    "words = [ dictionary[s] for s in odd_tokens ]\n",
    "output = \"\"\n",
    "count = 0\n",
    "for word in words:\n",
    "    output += word  + ' '\n",
    "    if len(output) > 100:\n",
    "        count += 1\n",
    "        if count > 20:\n",
    "            break\n",
    "        print(output)\n",
    "        output = ''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definitely a design choice not to tell the lemmatizer to\n",
    "remove proper names. I wanted to keep \"National Butterfly Center\" after all. But there are lot of other words which are \n",
    "not part of proper names.\n",
    "\n",
    "I can classify the language of the budget as basically:\n",
    "- Stop words in the usual NLP sense ('the', etc.)\n",
    "- Stop words in the government budget document sense ('pay', 'agency', 'fiscal', 'section', 'subsection', 'title', 'code')\n",
    "- And unique words that have to do with the specific thing being funded in one or possibly a few budget sections and not mentioned again.\n",
    "\n",
    "There are some more typical-looking words like 'beginning' and 'moderately' \n",
    "which you might think would come up more than once in a 200,000 word sample, but the federal budget is not a \n",
    "normal sample. \n",
    "\n",
    "If I were to drop out all the words mentioned in fewer than \n",
    "5 percent of the documents (for demonstration purposes, not\n",
    "for actual analysis), I'm sill only dropping 135 out of about \n",
    "4400 tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-02-27 21:05:26,409 : INFO : discarding 135 tokens: [('acquisition', 68), ('authorize', 368), ('capital', 72), ('derive', 81), ('excess', 66), ('advance', 69), ('appropriate', 340), ('committee', 233), ('current', 95), ('day', 163)]...\n",
      "2019-02-27 21:05:26,410 : INFO : keeping 4257 tokens which were in no less than 0 and no more than 62 (=5.0%) documents\n",
      "2019-02-27 21:05:26,413 : INFO : resulting dictionary: Dictionary(4257 unique tokens: ['aircraft', 'donation', 'exist', 'obtain', 'offset']...)\n"
     ]
    }
   ],
   "source": [
    "dictionary.filter_extremes(no_below=0, no_above=.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
